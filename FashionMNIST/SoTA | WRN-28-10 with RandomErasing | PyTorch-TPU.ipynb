{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yYiRmPPoVX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it-9JsMooaIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "outputId": "559bb04c-903f-4948-a476-c1b5a12fc175"
      },
      "source": [
        "VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3727  100  3727    0     0  45451      0 --:--:-- --:--:-- --:--:-- 45451\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Uninstalling torch-1.5.0a0+d6149a7:\n",
            "  Successfully uninstalled torch-1.5.0a0+d6149a7\n",
            "Uninstalling torchvision-0.6.0a0+3c254fb:\n",
            "  Successfully uninstalled torchvision-0.6.0a0+3c254fb\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.2)\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+e788e5b\n",
            "    Uninstalling torch-xla-1.6+e788e5b:\n",
            "      Successfully uninstalled torch-xla-1.6+e788e5b\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nteelfZaob7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "M, N = 4, 6\n",
        "RESULT_IMG_PATH = '/tmp/test_result.jpg'\n",
        "classes=[\n",
        "            \"T-shirt/top\",\n",
        "            \"Trouser\",\n",
        "            \"Pullover\",\n",
        "            \"Dress\",\n",
        "            \"Coat\",\n",
        "            \"Sandal\",\n",
        "            \"Shirt\",\n",
        "            \"Sneaker\",\n",
        "            \"Bag\",\n",
        "            \"Ankle boot\"]\n",
        "\n",
        "def plot_results(images, labels, preds):\n",
        "  images, labels, preds = images[:M*N], labels[:M*N], preds[:M*N]\n",
        "  inv_norm = transforms.Normalize(\n",
        "      mean=(-0.4914/0.2023, -0.4822/0.1994, -0.4465/0.2010),\n",
        "      std=(1/0.2023, 1/0.1994, 1/0.2010))\n",
        "\n",
        "  num_images = images.shape[0]\n",
        "  fig, axes = plt.subplots(M, N, figsize=(16, 9))\n",
        "  fig.suptitle('Correct / Predicted Labels (Red text for incorrect ones)')\n",
        "\n",
        "  for i, ax in enumerate(fig.axes):\n",
        "    ax.axis('off')\n",
        "    if i >= num_images:\n",
        "      continue\n",
        "    img, label, prediction = images[i], labels[i], preds[i]\n",
        "    img = inv_norm(img)\n",
        "    img = img.permute(1, 2, 0) # (C, M, N) -> (M, N, C)\n",
        "    label, prediction = label.item(), prediction.item()\n",
        "    if label == prediction:\n",
        "      ax.set_title(u'\\u2713', color='blue', fontsize=22)\n",
        "    else:\n",
        "      ax.set_title(\n",
        "          'X {}/{}'.format(classes[label],\n",
        "                          classes[prediction]), color='red')\n",
        "    ax.imshow(img)\n",
        "  plt.savefig(RESULT_IMG_PATH, transparent=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT__9Nu9omlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Parameters\n",
        "FLAGS = {}\n",
        "FLAGS['data_dir'] = \"/tmp/fashionmnist\"\n",
        "FLAGS['batch_size'] = 128\n",
        "FLAGS['num_workers'] = 4\n",
        "FLAGS['learning_rate'] = 0.1\n",
        "FLAGS['momentum'] = 0.9\n",
        "FLAGS['num_epochs'] = 300\n",
        "FLAGS['num_cores'] = 8\n",
        "FLAGS['log_steps'] = 20\n",
        "FLAGS['metrics_debug'] = True\n",
        "FLAGS['num_classes']=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMDBpQOiotQu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import lr_scheduler, SGD\n",
        "from torch.utils.data import Subset, Dataset, DataLoader\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "from torch_xla.core.xla_model import RateTracker\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.utils.utils as xu\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import FashionMNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "\n",
        "def train():\n",
        "    torch.manual_seed(1)\n",
        "    transform_train = transforms.Compose([\n",
        "            transforms.RandomCrop(28, padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "            transforms.RandomErasing(p = 0.5, scale = (0.02,0.4), ratio = (0.3,3.3), value=0.4914),\n",
        "        ])\n",
        "    \n",
        "    transform_test = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        ])\n",
        "    \n",
        "    train_dataset=FashionMNIST(root=os.path.join(FLAGS['data_dir'], str(xm.get_ordinal())),\n",
        "                       train=True, \n",
        "                       download=True, \n",
        "                       transform=transform_train)\n",
        "    \n",
        "    \n",
        "    \n",
        "    test_dataset=FashionMNIST(root=os.path.join(FLAGS['data_dir'], str(xm.get_ordinal())),\n",
        "                      train=False,\n",
        "                      download=False,\n",
        "                      transform=transform_test)\n",
        "    \n",
        "    train_sampler = DistributedSampler(\n",
        "          train_dataset,\n",
        "          num_replicas=xm.xrt_world_size(),\n",
        "          rank=xm.get_ordinal(),\n",
        "          shuffle=True)\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "          train_dataset,\n",
        "          batch_size=FLAGS['batch_size'],\n",
        "          sampler=train_sampler,\n",
        "          num_workers=FLAGS['num_workers'],\n",
        "          drop_last=True)\n",
        "      \n",
        "    test_loader = DataLoader(\n",
        "          test_dataset,\n",
        "          batch_size=FLAGS['batch_size'],\n",
        "          shuffle=False,\n",
        "          num_workers=FLAGS['num_workers'],\n",
        "          drop_last=True)\n",
        "    \n",
        "    \n",
        "    learning_rate = FLAGS['learning_rate'] * xm.xrt_world_size()\n",
        "    \n",
        "    device = xm.xla_device()\n",
        "    \n",
        "    classes=[\n",
        "            \"T-shirt/top\",\n",
        "            \"Trouser\",\n",
        "            \"Pullover\",\n",
        "            \"Dress\",\n",
        "            \"Coat\",\n",
        "            \"Sandal\",\n",
        "            \"Shirt\",\n",
        "            \"Sneaker\",\n",
        "            \"Bag\",\n",
        "            \"Ankle boot\"]\n",
        "    \n",
        "    print(device)\n",
        "    \n",
        "    class BasicBlock(nn.Module):\n",
        "        def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "            super(BasicBlock, self).__init__()\n",
        "            self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "            self.relu1 = nn.ReLU(inplace=True)\n",
        "            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                                   padding=1, bias=False)\n",
        "            self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "            self.relu2 = nn.ReLU(inplace=True)\n",
        "            self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                                   padding=1, bias=False)\n",
        "            self.droprate = dropRate\n",
        "            self.equalInOut = (in_planes == out_planes)\n",
        "            self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                                   padding=0, bias=False) or None\n",
        "        def forward(self, x):\n",
        "            if not self.equalInOut:\n",
        "                x = self.relu1(self.bn1(x))\n",
        "            else:\n",
        "                out = self.relu1(self.bn1(x))\n",
        "            out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "            if self.droprate > 0:\n",
        "                out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "            out = self.conv2(out)\n",
        "            return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "    \n",
        "    class NetworkBlock(nn.Module):\n",
        "        def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "            super(NetworkBlock, self).__init__()\n",
        "            self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "        def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "            layers = []\n",
        "            for i in range(nb_layers):\n",
        "                layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "            return nn.Sequential(*layers)\n",
        "        def forward(self, x):\n",
        "            return self.layer(x)\n",
        "    \n",
        "    class WideResNet(nn.Module):\n",
        "        def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
        "            super(WideResNet, self).__init__()\n",
        "            nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "            assert (depth - 4) % 6 == 0, 'depth should be 6n+4'\n",
        "            n = (depth - 4) // 6\n",
        "            block = BasicBlock\n",
        "            # 1st conv before any network block\n",
        "            self.conv1 = nn.Conv2d(1, nChannels[0], kernel_size=3, stride=1,\n",
        "                                   padding=1, bias=False)\n",
        "            # 1st block\n",
        "            self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "            # 2nd block\n",
        "            self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "            # 3rd block\n",
        "            self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "            # global average pooling and classifier\n",
        "            self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "            self.nChannels = nChannels[3]\n",
        "    \n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d):\n",
        "                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                    m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    m.weight.data.fill_(1)\n",
        "                    m.bias.data.zero_()\n",
        "                elif isinstance(m, nn.Linear):\n",
        "                    m.bias.data.zero_()\n",
        "                    \n",
        "        def forward(self, x):\n",
        "            out = self.conv1(x)\n",
        "            out = self.block1(out)\n",
        "            out = self.block2(out)\n",
        "            out = self.block3(out)\n",
        "            out = self.relu(self.bn1(out))\n",
        "            out = F.avg_pool2d(out, 7)\n",
        "            out = out.view(-1, self.nChannels)\n",
        "            return self.fc(out)\n",
        "        \n",
        "    model= WideResNet(num_classes=FLAGS['num_classes'], depth=28, widen_factor=10).to(device)\n",
        "    \n",
        "    loss_func=nn.CrossEntropyLoss()\n",
        "    optimizer=SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler=lr_scheduler.MultiStepLR(optimizer, milestones=[150,225], gamma=0.1)\n",
        "    \n",
        "    def train_loop_fn(loader):\n",
        "        tracker=RateTracker()\n",
        "        model.train()\n",
        "        print(\"Start Training\")\n",
        "        for counter, (images, labels) in enumerate(train_loader, start=1):\n",
        "            images=images.to(device)\n",
        "            labels=labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = loss_func(outputs, labels)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            tracker.add(FLAGS['batch_size'])\n",
        "            if counter % FLAGS['log_steps'] == 0:\n",
        "                print('[xla:{}]({}) Loss={:.5f} Rate={:.2f} GlobalRate={:.2f} Time={}'.format(\n",
        "                xm.get_ordinal(), counter, loss.item(), tracker.rate(),\n",
        "                tracker.global_rate(), time.asctime()), flush=True)\n",
        "            scheduler.step()\n",
        "    \n",
        "    def test_loop_fn(loader):\n",
        "        total_samples = 0\n",
        "        correct = 0\n",
        "        model.eval()\n",
        "        data, pred, target = None, None, None\n",
        "        for data, target in loader:\n",
        "            output = model(data)\n",
        "            pred = output.max(1, keepdim=True)[1]\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "            total_samples += data.size()[0]\n",
        "    \n",
        "        accuracy = 100.0 * correct / total_samples\n",
        "        print('[xla:{}] Accuracy={:.2f}%'.format(\n",
        "            xm.get_ordinal(), accuracy), flush=True)\n",
        "        return accuracy, data, pred, target\n",
        "    \n",
        "        # print(f'Epoch [{epoch+1}/{N_EPOCHS}] Loss= {(running_loss/counter)}')\n",
        "        # if((epoch+1)%125==0):\n",
        "        #     model.eval()\n",
        "        #     with torch.no_grad():\n",
        "        #         y_pred=[]\n",
        "        #         for images, labels in test_loader:\n",
        "        #             images = images.to(device)\n",
        "        #             labels = labels.to(device)\n",
        "        #             outputs = model(images)\n",
        "        #             # max returns (value ,index)\n",
        "        #             _, preds = torch.max(outputs, 1)\n",
        "        #             y_pred+=preds.tolist()\n",
        "        #         print(classification_report(test.targets, y_pred, target_names=classes))\n",
        "        #     model.train()\n",
        "        #     torch.save(model.state_dict(), DATA_DIR+\"/WRN-28-10_%d\"%(epoch+1))\n",
        "        # scheduler.step()\n",
        "    accuracy = 0.0\n",
        "    data, pred, target = None, None, None\n",
        "    for epoch in range(1, FLAGS['num_epochs'] + 1):\n",
        "        para_loader = pl.ParallelLoader(train_loader, [device])\n",
        "        train_loop_fn(para_loader.per_device_loader(device))\n",
        "        xm.master_print(\"Finished training epoch {}\".format(epoch))\n",
        "          \n",
        "        para_loader = pl.ParallelLoader(test_loader, [device])\n",
        "        accuracy, data, pred, target  = test_loop_fn(para_loader.per_device_loader(device))\n",
        "        if FLAGS['metrics_debug']:\n",
        "            xm.master_print(met.metrics_report(), flush=True)\n",
        "        scheduler.step()\n",
        "    PATH = FLAGS['data_dir']+\"/WRN-28-10_F.pth\"\n",
        "    torch.save(model.state_dict(), PATH)\n",
        "    return accuracy, data, pred, target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yvgdy2gowms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "090401af-9b10-4bd2-8dab-83f26a507e0c"
      },
      "source": [
        "# Start training processes\n",
        "def _mp_fn(rank, flags):\n",
        "    global FLAGS\n",
        "    FLAGS = flags\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "    accuracy, data, pred, target = train()\n",
        "    if rank == 0:\n",
        "       # Retrieve tensors that are on TPU core 0 and plot.\n",
        "       plot_results(data.cpu(), pred.cpu(), target.cpu())\n",
        "\n",
        "xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=FLAGS['num_cores'],\n",
        "          start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xla:1\n",
            "Start Training\n",
            "xla:0\n",
            "xla:0\n",
            "xla:0\n",
            "xla:0\n",
            "xla:0\n",
            "xla:0\n",
            "xla:0\n",
            "Start Training\n",
            "Start Training\n",
            "Start Training\n",
            "Start Training\n",
            "Start Training\n",
            "Start Training\n",
            "Start Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8M1rcr1joyi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "img = cv2.imread(RESULT_IMG_PATH, cv2.IMREAD_UNCHANGED)\n",
        "cv2_imshow(img)\n",
        "\n",
        "\n",
        "print('Finished Training..')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OLA3crlo1lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}